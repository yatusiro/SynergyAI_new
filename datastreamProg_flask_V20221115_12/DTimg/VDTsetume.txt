对于多元分类问题，class_names应该是一个包含所有可能类别的列表。然而，如果你的目标变量（在你的例子中是温度）是一个连续的值，那么你的问题就不再是一个分类问题，而是一个回归问题。

在处理回归问题时，决策树模型的实现方法与分类问题有所不同。sklearn库为此提供了DecisionTreeRegressor类。因此，你无需指定class_names，因为回归问题的输出是一个连续的数值，而不是一个离散的类别。

然而，如果你希望将温度值转化为一个分类问题（例如，将温度值转化为“低”、“中”和“高”三个类别），你可以通过创建一个新的类别变量来实现。在这种情况下，你需要将这些类别的名称提供给class_names参数。

在你的特定情况下，如果你的目标变量是一个连续的温度值，你应该考虑使用DecisionTreeRegressor来解决你的问题，而不是DecisionTreeClassifier。

在决策树中，Gini指数是一种用于评估分裂质量的指标。它是由卡洛·吉尼（Carlo Gini）提出的，用于衡量一个国家收入差距的指数，但在决策树中它的定义有所不同。

在决策树算法中，Gini指数用于衡量数据的纯度或不确定性。如果一个节点的所有数据都属于同一类别，我们称这个节点是“纯”的，其Gini指数为0。如果数据均匀地分布在各个类别中（即节点的不确定性最大），那么Gini指数将达到最大值。

对于一个二分类问题，假设某个节点的数据中，正类的比例为p，那么这个节点的Gini指数可以计算为：

Gini = 1 - (p^2 + (1-p)^2)

你可以看到，如果p=0或1（即所有数据都属于同一类别），那么Gini指数为0；如果p=0.5（即数据均匀地分布在两个类别中），那么Gini指数为0.5，达到最大值。

决策树算法会尝试找到可以最大程度地降低Gini指数的特征和阈值来进行分裂。每次分裂后，子节点的Gini指数通常会比父节点低，也就是说，子节点的数据通常比父节点更“纯”。

在决策树的可视化中，每个节点上的Gini值表示了在进行分裂前，该节点的数据的不确定性或混乱程度。